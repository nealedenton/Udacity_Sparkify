{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Wrangling Data\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'mini_sparkify_event_data.json'\n",
    "\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Look at the schema\n",
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>auth</th>\n",
       "      <th>firstName</th>\n",
       "      <th>gender</th>\n",
       "      <th>itemInSession</th>\n",
       "      <th>lastName</th>\n",
       "      <th>length</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>method</th>\n",
       "      <th>page</th>\n",
       "      <th>registration</th>\n",
       "      <th>sessionId</th>\n",
       "      <th>song</th>\n",
       "      <th>status</th>\n",
       "      <th>ts</th>\n",
       "      <th>userAgent</th>\n",
       "      <th>userId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Martha Tilston</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>277.89016</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Rockpools</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352117000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Five Iron Frenzy</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>Long</td>\n",
       "      <td>236.09424</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352180000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adam Lambert</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>282.82730</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Time For Miracles</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352394000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enigma</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>80</td>\n",
       "      <td>Long</td>\n",
       "      <td>262.71302</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Knocking On Forbidden Doors</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352416000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daft Punk</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>52</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>223.60771</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Harder Better Faster Stronger</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352676000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The All-American Rejects</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>81</td>\n",
       "      <td>Long</td>\n",
       "      <td>208.29995</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Don't Leave Me</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352678000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Velvet Underground / Nico</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Micah</td>\n",
       "      <td>M</td>\n",
       "      <td>82</td>\n",
       "      <td>Long</td>\n",
       "      <td>260.46649</td>\n",
       "      <td>free</td>\n",
       "      <td>Boston-Cambridge-Newton, MA-NH</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538332e+12</td>\n",
       "      <td>8</td>\n",
       "      <td>Run Run Run</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352886000</td>\n",
       "      <td>\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Starflyer 59</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>53</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>185.44281</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Passengers (Old Album Version)</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352899000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>54</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>Add to Playlist</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>1538352905000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Frumpies</td>\n",
       "      <td>Logged In</td>\n",
       "      <td>Colin</td>\n",
       "      <td>M</td>\n",
       "      <td>55</td>\n",
       "      <td>Freeman</td>\n",
       "      <td>134.47791</td>\n",
       "      <td>paid</td>\n",
       "      <td>Bakersfield, CA</td>\n",
       "      <td>PUT</td>\n",
       "      <td>NextSong</td>\n",
       "      <td>1.538173e+12</td>\n",
       "      <td>29</td>\n",
       "      <td>Fuck Kitty</td>\n",
       "      <td>200</td>\n",
       "      <td>1538353084000</td>\n",
       "      <td>Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          artist       auth firstName gender  itemInSession  \\\n",
       "0                 Martha Tilston  Logged In     Colin      M             50   \n",
       "1               Five Iron Frenzy  Logged In     Micah      M             79   \n",
       "2                   Adam Lambert  Logged In     Colin      M             51   \n",
       "3                         Enigma  Logged In     Micah      M             80   \n",
       "4                      Daft Punk  Logged In     Colin      M             52   \n",
       "5       The All-American Rejects  Logged In     Micah      M             81   \n",
       "6  The Velvet Underground / Nico  Logged In     Micah      M             82   \n",
       "7                   Starflyer 59  Logged In     Colin      M             53   \n",
       "8                           None  Logged In     Colin      M             54   \n",
       "9                       Frumpies  Logged In     Colin      M             55   \n",
       "\n",
       "  lastName     length level                        location method  \\\n",
       "0  Freeman  277.89016  paid                 Bakersfield, CA    PUT   \n",
       "1     Long  236.09424  free  Boston-Cambridge-Newton, MA-NH    PUT   \n",
       "2  Freeman  282.82730  paid                 Bakersfield, CA    PUT   \n",
       "3     Long  262.71302  free  Boston-Cambridge-Newton, MA-NH    PUT   \n",
       "4  Freeman  223.60771  paid                 Bakersfield, CA    PUT   \n",
       "5     Long  208.29995  free  Boston-Cambridge-Newton, MA-NH    PUT   \n",
       "6     Long  260.46649  free  Boston-Cambridge-Newton, MA-NH    PUT   \n",
       "7  Freeman  185.44281  paid                 Bakersfield, CA    PUT   \n",
       "8  Freeman        NaN  paid                 Bakersfield, CA    PUT   \n",
       "9  Freeman  134.47791  paid                 Bakersfield, CA    PUT   \n",
       "\n",
       "              page  registration  sessionId                            song  \\\n",
       "0         NextSong  1.538173e+12         29                       Rockpools   \n",
       "1         NextSong  1.538332e+12          8                          Canada   \n",
       "2         NextSong  1.538173e+12         29               Time For Miracles   \n",
       "3         NextSong  1.538332e+12          8     Knocking On Forbidden Doors   \n",
       "4         NextSong  1.538173e+12         29   Harder Better Faster Stronger   \n",
       "5         NextSong  1.538332e+12          8                  Don't Leave Me   \n",
       "6         NextSong  1.538332e+12          8                     Run Run Run   \n",
       "7         NextSong  1.538173e+12         29  Passengers (Old Album Version)   \n",
       "8  Add to Playlist  1.538173e+12         29                            None   \n",
       "9         NextSong  1.538173e+12         29                      Fuck Kitty   \n",
       "\n",
       "   status             ts                                          userAgent  \\\n",
       "0     200  1538352117000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "1     200  1538352180000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "2     200  1538352394000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "3     200  1538352416000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "4     200  1538352676000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "5     200  1538352678000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "6     200  1538352886000  \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK...   \n",
       "7     200  1538352899000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "8     200  1538352905000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "9     200  1538353084000  Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) G...   \n",
       "\n",
       "  userId  \n",
       "0     30  \n",
       "1      9  \n",
       "2     30  \n",
       "3      9  \n",
       "4     30  \n",
       "5      9  \n",
       "6      9  \n",
       "7     30  \n",
       "8     30  \n",
       "9     30  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at some rows using pandas for more readable output\n",
    "user_log.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many rows do we have?\n",
    "user_log.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: artist (string) has 17656 unique values\n",
      "Column: auth (string) has 4 unique values\n",
      "Column: firstName (string) has 190 unique values\n",
      "Column: gender (string) has 3 unique values\n",
      "Column: itemInSession (bigint) has 1322 unique values\n",
      "Column: lastName (string) has 174 unique values\n",
      "Column: length (double) has 14866 unique values\n",
      "Column: level (string) has 2 unique values\n",
      "Column: location (string) has 115 unique values\n",
      "Column: method (string) has 2 unique values\n",
      "Column: page (string) has 22 unique values\n"
     ]
    }
   ],
   "source": [
    "#Find out number of unique values per column and their datatypes\n",
    "columns = user_log.schema.fieldNames()\n",
    "datatypes = pd.DataFrame(user_log.dtypes, columns = ['column', 'dtype'])\n",
    "\n",
    "for col in columns:\n",
    "    nunique = user_log.select(col).dropDuplicates().count()\n",
    "    dtype = datatypes[datatypes['column'] == col]['dtype'].values[0]\n",
    "    output = 'Column: {} ({}) has {} unique values'.format(col, dtype, nunique)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the top artists\n",
    "user_log.groupBy(\"artist\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the top auth\n",
    "user_log.groupBy(\"auth\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the top gender\n",
    "user_log.groupBy(\"gender\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top  itemInSession\n",
    "user_log.groupBy(\"itemInSession\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top  level\n",
    "user_log.groupBy(\"level\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top  location\n",
    "user_log.groupBy(\"location\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top method\n",
    "user_log.groupBy(\"method\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top pages\n",
    "user_log.groupBy(\"page\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top registration\n",
    "user_log.groupBy(\"registration\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top status\n",
    "user_log.groupBy(\"status\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top userAgent\n",
    "user_log.groupBy(\"userAgent\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Look at the top userId\n",
    "user_log.groupBy(\"userId\").count().sort(desc(\"count\")).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We appear to have several thousand rows with missinng userId let's look at those pages\n",
    "missing_userIds = user_log.select([\"page\", \"level\"]).where(user_log.userId == \"\")\n",
    "missing_userIds.groupby(\"page\").count().sort(desc(\"count\")).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page names of the rows with missing Ids suggest this is probably activity prior to the user logging in.  We don't have any other kind of secondary ID, so we cannot attempt to associate these rows with an actual user.  The userAgent field isn't specific enough.  Because of this we must simply drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create user_log_valid - removing rows without userId\n",
    "user_log_valid = user_log.filter(user_log[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are interested in identifying when users cancel their account.  Create a function to flag this\n",
    "flag_cancellation_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use function to flag cancellation rows\n",
    "user_log_valid = user_log_valid.withColumn(\"cancelled\", flag_cancellation_event(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.toPandas().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create grouped by dataframe\n",
    "users_grouped = user_log_valid.groupby(\"userId\").sum(\"cancelled\").sort(desc(\"sum(cancelled)\"))#.toPandas().head(10)\n",
    "#Check we have removed the missing userId\n",
    "#users_grouped.count().sort(desc(\"count\")).toPandas().head(10)\n",
    "users_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_grouped = user_log_valid.groupby(\"userId\")\\\n",
    "                    .agg({\"cancelled\" : \"sum\",\n",
    "                         \"page\" : \"count\",\n",
    "                         \"gender\" : 'approx_count_distinct'})\\\n",
    "                    .withColumnRenamed(\"sum(cancelled)\", \"money\")\n",
    "users_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of all the distinct page names\n",
    "pages_list = list(user_log_valid.select(\"page\").dropDuplicates().toPandas()[\"page\"])\n",
    "pages_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepend \"_event\" to each page name for use as columns\n",
    "pages_columns_renamed = [\"event_\" + x.replace(\" \", \"_\") for x in pages_list]\n",
    "pages_columns_renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe of userId pivoted by page, to create a column of page views for each page \n",
    "user_page_events = user_log_valid.groupby(\"userId\").pivot(\"page\", pages_list).count()\n",
    "#Replace NaN values with 0\n",
    "user_page_events = user_page_events.fillna(0)\n",
    "#Rename the columns, so each is prepended with \"_event\"\n",
    "user_page_events = user_page_events.toDF(* [\"userId\"] + pages_columns_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_page_events.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_page_events.toPandas().groupby(\"event_Cancellation_Confirmation\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.sort(['userId','sessionId']).toPandas().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select([\"userId\", \"firstname\", \"page\", \"song\"]).where(user_log.userId == \"1046\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.groupby([\"page\"]).count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pages\n",
    "user_log.groupBy(\"page\").count().withColumnRenamed(\"count\",\"distinct_name\").sort(desc(\"count\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'''\n",
    "def check_columns_map_one_to_one(df, col_to_group_by, col_to_check_distincts):\n",
    "    count_of_group_by_with_more_than_one = df.groupby([col_to_group_by]).\\\n",
    "        agg(countDistinct(col_to_check_distincts).\\\n",
    "        alias('unique_count')).\\\n",
    "        filter(\"unique_count > 1\").count()\n",
    "    return (count_of_group_by_with_more_than_one == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are several columns which sound like they might map 1-1 with a userId. \n",
    "#Check each column, if it does map 1-1 it is a candidate for being used as a feature\n",
    "cols_to_check_are_unique_per_userId = ['gender', 'level', 'userAgent', 'status', 'registration', 'method', 'auth', 'location']\n",
    "\n",
    "for col in cols_to_check_are_unique_per_userId:\n",
    "    unique_check = check_columns_map_one_to_one(user_log_valid, 'userId', col)\n",
    "    print('Every userId has a unique value for {} : {}'.format(col, unique_check))\n",
    "    \n",
    "#gender, userAgent, registraion and location all map 1-1 with a userId so are all candidate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_lookup = user_log_valid.groupBy(\"userId\", \"gender\").count().select(\"userId\", \"gender\")\n",
    "\n",
    "is_female = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "\n",
    "gender_lookup = gender_lookup.withColumn(\"is_female\", is_female(\"gender\")).select(\"userId\", \"is_female\")\n",
    "gender_lookup.toPandas().head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "INPUT:\n",
    "df\n",
    "\n",
    "OUPUT:\n",
    "df\n",
    "'''\n",
    "def engineer_features(df):\n",
    "    \n",
    "    #UDF for getting date string from timestamp\n",
    "    get_date = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d'))\n",
    "    days_diff = udf(lambda x, y: (datetime.datetime.fromtimestamp(x / 1000.0)- datetime.datetime.fromtimestamp(y / 1000.0)).days)\n",
    "    \n",
    "    \n",
    "    #Create date column from timestamp \"YYYY-MM-DD\" format\n",
    "    df = df.withColumn(\"date\", get_date(df.ts))\n",
    "    \n",
    "    \n",
    "    #Create list of all the distinct page names\n",
    "    pages_list = list(df.select(\"page\").dropDuplicates().toPandas()[\"page\"])\n",
    "    #Prepend \"_event\" to each page name for use as columns\n",
    "    pages_columns_renamed = [\"event_\" + x.replace(\" \", \"_\") for x in pages_list]\n",
    "    \n",
    "    #Create dataframe of userId pivoted by page, to create a column of page views for each page \n",
    "    user_page_events = df.groupby(\"userId\").pivot(\"page\", pages_list).count()\n",
    "    #Replace NaN values with 0\n",
    "    user_page_events = user_page_events.fillna(0)\n",
    "    #Rename the columns, so each is prepended with \"_event\"\n",
    "    user_page_events = user_page_events.toDF(* [\"userId\"] + pages_columns_renamed)\n",
    "    \n",
    "    #The cancellation confirmation event is what we are trying to predict, we rename this \"label\"\n",
    "    user_page_events= user_page_events.withColumnRenamed(\"event_Cancellation_Confirmation\",\"label\")\n",
    "    \n",
    "    gender_lookup = df.groupBy(\"userId\", \"gender\").count().select(\"userId\", \"gender\")\n",
    "    \n",
    "   \n",
    "    #create UDF to map gender (F|M) to 1|0\n",
    "    is_female = udf(lambda x: 1 if x == \"F\" else 0, IntegerType())\n",
    "    #Map gender as 1|o in is_female column, drop gender column\n",
    "    gender_lookup = gender_lookup.withColumn(\"is_female\", is_female(\"gender\")).select(\"userId\", \"is_female\")\n",
    "    \n",
    "    #Join our userId feature tables together\n",
    "    df_features = gender_lookup.join(user_page_events, on=\"userId\", how=\"inner\")\n",
    "    \n",
    "    #Create session count and 404 errors\n",
    "    count_condition = lambda cond: f.sum(f.when(cond, 1).otherwise(0))\n",
    "    df_user_features = df.groupBy(\"userId\").agg(\n",
    "        f.first('registration').alias('registration_ts'),\n",
    "        countDistinct('sessionId').alias('sessions'),\n",
    "        countDistinct('date').alias('unique_dates'),\n",
    "        count_condition(f.col('status') == '404').alias('404_errors')\n",
    "    )\n",
    "    \n",
    "    #Join session count and 404 errors\n",
    "    df_features = df_features.join(df_user_features, on=\"userId\", how=\"inner\")\n",
    "    \n",
    "    \n",
    "    #Group by userId and sessionId to be able to calculate average hits per session\n",
    "    df_user_sessions = df.groupBy(\"userId\", \"sessionId\").agg(\n",
    "        countDistinct('itemInSession').alias('hitsInSession')\n",
    "    )\n",
    "    \n",
    "    #Calculate average hits per session\n",
    "    df_hits_per_session = df_user_sessions.groupBy(\"userId\").agg(\n",
    "        f.avg(\"hitsInSession\").alias(\"hitsInSession_mean\")\n",
    "    )\n",
    "    \n",
    "    #Join hits_per_session to other features\n",
    "    df_features = df_features.join(df_hits_per_session, on=\"userId\", how=\"inner\")\n",
    "    \n",
    "    #Get earliest timestamp in whole data set\n",
    "    min_timestamp = df.agg({'ts': 'min'}).head()[0]\n",
    "    \n",
    "    #Convert registration timestamp to a date string \"YYYY-MM-DD\"\n",
    "    df_features = df_features.withColumn(\"registration_date\", get_date(df_features.registration_ts)).\\\n",
    "        withColumn(\"diff\", f.round((f.lit(min_timestamp) - df_features.registration_ts)/1000/60/60/24))\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def engineer_features2(df):\n",
    "    \n",
    "    count_condition = lambda cond: f.sum(f.when(cond, 1).otherwise(0))\n",
    "    \n",
    "    df_session_and_404 = df.groupBy(\"userId\").agg(\n",
    "        countDistinct('sessionId').alias('sessions'),\n",
    "        count_condition(f.col('status') == '404').alias('404_errors')\n",
    "    )\n",
    "    \n",
    "    df_user_sessions = df.groupBy(\"userId\", \"sessionId\").agg(\n",
    "        countDistinct('itemInSession').alias('hitsInSession')\n",
    "    )\n",
    "    \n",
    "    df_hits_per_session = df_user_sessions.groupBy(\"userId\").agg(\n",
    "        f.avg(\"hitsInSession\").alias(\"hitsInSession_mean\")\n",
    "    )\n",
    "            \n",
    "    \n",
    "    return df_hits_per_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_valid.groupBy(\"userId\").agg({'registration' : 'first'}).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageCount = (user_log_valid\n",
    "                .groupBy().mean()).head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = 1538348400000\n",
    "\n",
    "get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)\n",
    "get_date = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d'))\n",
    "days_diff = udf(lambda x, y: (datetime.datetime.fromtimestamp(x / 1000.0)- datetime.datetime.fromtimestamp(y / 1000.0).days))\n",
    "\n",
    "                \n",
    "user_log_time = user_log_valid. \\\n",
    "    withColumn(\"date\", get_date(user_log_valid.ts)). \\\n",
    "    withColumn(\"date_registration\", get_date(user_log_valid.registration)).\\\n",
    "    withColumn(\"diff\", f.round((f.lit(min_date) - user_log_valid.registration)/1000/60/60/24))\n",
    "    #withColumn(\"days_since_reg\", days_diff(f.lit(min_date), user_log_valid.registration)).collect()\n",
    "\n",
    "\n",
    "\n",
    "#newdf = df.select(to_date(‘date_from’, ‘yyyy-MM-dd HH:mm:ss’).alias(‘from_date’), to_date(‘date_to’, ‘yyyy-MM-dd HH:mm:ss’).alias(‘to_date’), )\n",
    "#newdf.withColumn(“diff_days”, F.datediff(“to_date”, “from_date”)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_log_time.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.datetime.fromtimestamp(1538173362000 / 1000).strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_log_time.groupby('date').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = engineer_features(user_log_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.toPandas().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feature_columns = df_features.columns\n",
    "numeric_feature_columns.remove('userId')\n",
    "numeric_feature_columns.remove('label')\n",
    "numeric_feature_columns.remove('registration_ts')\n",
    "numeric_feature_columns.remove('registration_date')\n",
    "numeric_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assemble the Vector of Numeric features\n",
    "assembler = VectorAssembler(inputCols=numeric_feature_columns, outputCol=\"NumFeatures\")\n",
    "df = assembler.transform(df_features)\n",
    "#df.head() \n",
    "\n",
    "#Normalize the Vectors\n",
    "scaler = Normalizer(inputCol=\"NumFeatures\", outputCol=\"ScaledNumFeatures\")\n",
    "df =  scaler.transform(df)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.select(\"label\",\"ScaledNumFeatures\").withColumnRenamed('ScaledNumFeatures','features')\n",
    "#data.toPandas().head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Training\n",
    "\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.0)\n",
    "model = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coefficientMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 way split splits\n",
    "#train, rest = df.randomSplit([0.6, 0.4], seed=40)\n",
    "#test, validation = rest.randomSplit([0.5, 0.5], seed =40)\n",
    "#train is now 60%, test is 20% and validation is 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 way split\n",
    "train, test = df_features.drop('userId').randomSplit([0.8, 0.2], seed=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numeric_feature_columns, outputCol=\"NumFeatures\")\n",
    "scaler = Normalizer(inputCol=\"NumFeatures\", outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=5, regParam=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "pipeline_model = pipeline.fit(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#important: need to cast to float type, and order by prediction, else it won't work\n",
    "predictions_and_labels = predictions.select(['prediction','label']).withColumn('label', f.col('label').cast(FloatType())).orderBy('prediction')\n",
    "\n",
    "#select only prediction and label columns\n",
    "predictions_and_labels = predictions_and_labels.select(['prediction','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(predictions_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = predictions.select(['label']).collect()\n",
    "y_pred = predictions.select(['prediction']).collect()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.filter(predictions.label == predictions.prediction).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crossval = CrossValidator(estimator = pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
